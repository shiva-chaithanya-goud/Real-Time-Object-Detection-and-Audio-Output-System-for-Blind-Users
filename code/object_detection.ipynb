{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64169a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on your left side there are :\n",
      "1: bowl\n",
      "2: person\n",
      "1: wine-glass\n",
      " on your right side there are :\n",
      "2: bowl\n",
      "1: cup\n",
      "5: person\n",
      "2: wine-glass\n",
      " on your front side there are :\n",
      "2: cup\n",
      "2: person\n",
      "2: wine-glass\n",
      " on your back side there are :\n",
      "3: cup\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Open the input image\n",
    "input_image = Image.open('two.png')\n",
    "\n",
    "# Get the width and height of the input image\n",
    "width, height = input_image.size\n",
    "\n",
    "# Calculate the width of each cropped image\n",
    "crop_width = width // 8\n",
    "\n",
    "# Loop through 8 times to crop the image into 8 vertical pieces\n",
    "for i in range(8):\n",
    "    # Define the cropping box for the current piece (left, upper, right, lower)\n",
    "    cropping_box = (i * crop_width, 0, (i+1) * crop_width, height)\n",
    "    \n",
    "    # Crop the image using the defined box\n",
    "    output_image = input_image.crop(cropping_box)\n",
    "    \n",
    "    # Save the cropped image\n",
    "    output_image.save(f'image{i}.png')\n",
    "\n",
    "\n",
    "image1 = Image.open(\"image7.png\")\n",
    "image2 = Image.open(\"image0.png\")\n",
    "image3 = Image.open(\"image1.png\")\n",
    "image4 = Image.open(\"image2.png\")\n",
    "image5 = Image.open(\"image3.png\")\n",
    "image6 = Image.open(\"image4.png\")\n",
    "image7 = Image.open(\"image5.png\")\n",
    "image8 = Image.open(\"image6.png\")\n",
    "\n",
    "\n",
    "width1, height1 = image1.size\n",
    "width2, height2 = image2.size\n",
    "\n",
    "width3, height3 = image3.size\n",
    "width4, height4 = image4.size\n",
    "\n",
    "width5, height5 = image5.size\n",
    "width6, height6 = image6.size\n",
    "\n",
    "width7, height7 = image7.size\n",
    "width8, height8 = image8.size\n",
    "\n",
    "\n",
    "total_width1 = width1 + width2\n",
    "total_width3 = width3 + width4\n",
    "\n",
    "total_width5 = width5 + width6\n",
    "total_width7 = width7 + width8\n",
    "\n",
    "\n",
    "\n",
    "output_image1 = Image.new('RGB', (total_width1, height1))\n",
    "output_image3 = Image.new('RGB', (total_width3, height3))\n",
    "output_image5 = Image.new('RGB', (total_width5, height5))\n",
    "output_image7 = Image.new('RGB', (total_width7, height7))\n",
    "\n",
    "\n",
    "output_image1.paste(image1, (0, 0))\n",
    "output_image1.paste(image2, (width1, 0))\n",
    "output_image1.save(\"back_image.png\")\n",
    "\n",
    "output_image3.paste(image3, (0, 0))\n",
    "output_image3.paste(image4, (width3, 0))\n",
    "output_image3.save(\"left_image.png\")\n",
    "\n",
    "output_image5.paste(image5, (0, 0))\n",
    "output_image5.paste(image6, (width5, 0))\n",
    "output_image5.save(\"front_image.png\")\n",
    "\n",
    "output_image7.paste(image7, (0, 0))\n",
    "output_image7.paste(image8, (width7, 0))\n",
    "output_image7.save(\"right_image.png\")\n",
    "\n",
    "\n",
    "# Load the pre-trained YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "# Load the object class names\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Set the minimum probability threshold for object detection\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Set the non-maximum suppression threshold for overlapping boxes\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# Load the four images for object detection\n",
    "image_files = [\"left_image.png\", \"right_image.png\", \"front_image.png\", \"back_image.png\"]\n",
    "sides= [\"left side\",\"right side\",\"front side\",\"back side\"]\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    image = cv2.imread(image_file)\n",
    "    images.append(image)\n",
    "text=\"\"\n",
    "# Loop through each image and perform object detection\n",
    "for i, image in enumerate(images):\n",
    "    # Prepare the image for object detection\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Set the input for the neural network\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Forward pass through the network\n",
    "    output_layers = net.getUnconnectedOutLayersNames()\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "\n",
    "    # Process the output layer by layer\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            # Extract the class scores and box coordinates\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confidence_threshold:\n",
    "                # Calculate the box coordinates and add to the list\n",
    "                box = detection[:4] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "                (center_x, center_y, width, height) = box.astype(\"int\")\n",
    "                x = int(center_x - (width / 2))\n",
    "                y = int(center_y - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maximum suppression to remove overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "    list1=[]\n",
    "    # Print the list of objects detected for the current image\n",
    "    if len(indices) > 0:\n",
    "        text +=\" on your \"+sides[i]+\" there are \"+ \":\\n\"\n",
    "        for j in indices.flatten():\n",
    "            #print(classes[class_ids[j]], confidences[j])\n",
    "            list1.append(classes[class_ids[j]])\n",
    "        \n",
    "        count_dict=Counter(list1)\n",
    "        for obj in sorted(count_dict):\n",
    "            text +=f\"{count_dict[obj]}: {obj}\\n\"\n",
    "    else:\n",
    "        text+=\" there are No objects on your \"+ sides[i]+\"\\n\"\n",
    "        \n",
    "import pyttsx3\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Define output text\n",
    "#text = \"Objects detected in image left_image.png : clock 1, person 5. Objects detected in image right_image.png : clock 1, person 6. No objects detected in image front_image.png. Objects detected in image back_image.png : person 6.\"\n",
    "\n",
    "# Set properties for the text-to-speech engine\n",
    "engine.setProperty('rate', 150)\n",
    "engine.setProperty('volume', 1)\n",
    "\n",
    "# Speak the output text\n",
    "engine.say(text)\n",
    "engine.runAndWait()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8ae00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89b34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
